{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPj+q1iW4yBbuox2ZwHLkvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnanelhayani/Lab2/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmOjYUZBBZbw",
        "outputId": "dffddc75-762f-4951-f00d-cbf272d03063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Vérification de la disponibilité de GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Téléchargement et transformation des données\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYw-OAuCBu1o",
        "outputId": "cbb5ca7e-5f46-46c1-f7df-422262b9d4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 486kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.38MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.30MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n"
      ],
      "metadata": {
        "id": "vqOfbiwVBy1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "5k7vKRKuB0pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZKNgxiDB5EO",
        "outputId": "bf85294a-e415-4401-f91e-13d168257f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2536\n",
            "Epoch [2/10], Loss: 0.0872\n",
            "Epoch [3/10], Loss: 0.0663\n",
            "Epoch [4/10], Loss: 0.0539\n",
            "Epoch [5/10], Loss: 0.0445\n",
            "Epoch [6/10], Loss: 0.0405\n",
            "Epoch [7/10], Loss: 0.0345\n",
            "Epoch [8/10], Loss: 0.0322\n",
            "Epoch [9/10], Loss: 0.0269\n",
            "Epoch [10/10], Loss: 0.0245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
        "    return accuracy, f1\n",
        "\n",
        "evaluate_model(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0rcIHGyB9GA",
        "outputId": "5e5a0236-9f3e-494e-fa2d-2894206ddc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9904, F1 Score: 0.9904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9904, 0.9903961347598148)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "\n"
      ],
      "metadata": {
        "id": "VBuV-l2cDHJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, mnist_dataset):\n",
        "        self.mnist_dataset = mnist_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.mnist_dataset[idx]\n",
        "\n",
        "        # Convertir l'image en Tensor\n",
        "        image = F.to_tensor(image)\n",
        "\n",
        "        # Simuler une boîte englobante (bbox) pour la détection\n",
        "        bbox = torch.tensor([[5, 5, 20, 20]], dtype=torch.float32)  # Box fictive pour démonstration\n",
        "\n",
        "        # Préparer les cibles\n",
        "        target = {\n",
        "            \"boxes\": bbox,  # Une seule bbox pour MNIST\n",
        "            \"labels\": torch.tensor([label], dtype=torch.int64),  # Classe associée\n",
        "        }\n",
        "\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "vRw0XBV-GKj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_subset(dataset, fraction=0.1, seed=42):\n",
        "    random.seed(seed)\n",
        "    indices = random.sample(range(len(dataset)), int(len(dataset) * fraction))\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "# Charger le dataset complet\n",
        "full_train_dataset = MNIST(root='./data', train=True, download=True)\n",
        "full_test_dataset = MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "# Créer des sous-ensembles (10% des données)\n",
        "train_dataset = create_subset(full_train_dataset, fraction=0.1)\n",
        "test_dataset = create_subset(full_test_dataset, fraction=0.1)\n",
        "\n",
        "# Adapter les sous-ensembles pour la détection d'objets\n",
        "train_detection_dataset = MNISTDetectionDataset(train_dataset)\n",
        "test_detection_dataset = MNISTDetectionDataset(test_dataset)\n",
        "\n",
        "# Créer les DataLoaders\n",
        "train_loader = DataLoader(train_detection_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(test_detection_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
      ],
      "metadata": {
        "id": "LYzj4Am2GQ1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Charger le modèle pré-entraîné\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Ajuster le nombre de classes (10 pour MNIST + 1 pour la classe \"fond\")\n",
        "num_classes = 10 + 1\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Envoyer le modèle sur le GPU ou le CPU\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsySR81bGVhf",
        "outputId": "aef2c239-fd0c-4702-e3e5-f4edd1ffd3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 167MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamètres\n",
        "num_epochs = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Optimiseur\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n"
      ],
      "metadata": {
        "id": "CS0-2UIFGfp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, device, num_epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for images, targets in train_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Calcul de la perte\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            # Rétropropagation\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += losses.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, optimizer, device, num_epochs=num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4196NVJGj4l",
        "outputId": "5ed966a1-c473-4aa8-9410-81d7743b8ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Loss: 0.0743\n",
            "Epoch [2/3], Loss: 0.0218\n",
            "Epoch [3/3], Loss: 0.0136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Extraire les prédictions\n",
        "            for i, output in enumerate(outputs):\n",
        "                y_true.append(targets[i]['labels'][0].item())\n",
        "                if len(output['labels']) > 0:  # Si une prédiction est faite\n",
        "                    y_pred.append(output['labels'][0].item())\n",
        "                else:  # Si aucune prédiction\n",
        "                    y_pred.append(-1)  # Classe spéciale pour \"aucune prédiction\"\n",
        "\n",
        "    # Calcul des métriques\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "evaluate_model(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5qu8N62G3dA",
        "outputId": "32841513-a8a3-4821-82a1-3118a0e64a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8960, F1 Score: 0.8608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Question 3 : Comparaison entre CNN et Faster R-CNN\n",
        "\n",
        "| Métrique               | CNN                        | Faster R-CNN                        |\n",
        "|------------------------|----------------------------|-------------------------------------|\n",
        "| Précision (Accuracy)    | 0,9904 (99,04%)            | 0,8960 (89,60%)                    |\n",
        "| Score F1               | 0,9904 (99,04%)            | 0,8608 (86,08%)                    |\n",
        "| Perte (Loss)           | Faible (mieux)             | Élevée (surcoût de détection)      |\n",
        "| Temps d'entraînement   | Rapide (quelques minutes)  | Plus lent (50-60 minutes)           |\n",
        "\n",
        "```\n",
        "Observations :\n",
        "\n",
        "\n",
        "1.   Précision et Score F1 : Le modèle CNN surpasse le Faster R-CNN dans les tâches de classification car son architecture est conçue pour ces types de problèmes. En revanche, Faster R-CNN est plus adapté à la détection d'objets.\n",
        "2.   Perte (Loss) : Le CNN obtient une perte plus faible, car il est optimisé uniquement pour la classification, contrairement à Faster R-CNN, qui nécessite des calculs supplémentaires liés à la détection d'objets.\n",
        "3.   Temps d'entraînement : Faster R-CNN prend beaucoup plus de temps à s'entraîner en raison de sa complexité architecturale (réseaux de propositions de régions, etc.).\n",
        "4.   Adaptabilité : Pour une tâche de classification comme MNIST, le CNN est le choix le plus approprié.\n"
      ],
      "metadata": {
        "id": "RmM6d7J8XtK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Définir l'appareil (CPU dans ce cas)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Définir les transformations pour les images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Redimensionner les images\n",
        "    transforms.ToTensor(),  # Convertir en tensor\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Répéter le canal pour avoir 3 canaux\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalisation\n",
        "])\n",
        "\n",
        "# Charger les ensembles de données MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Sélectionner un sous-ensemble de 10% des données\n",
        "train_size = int(0.1 * len(train_dataset))\n",
        "test_size = int(0.1 * len(test_dataset))\n",
        "\n",
        "train_subset, test_subset = torch.utils.data.random_split(train_dataset, [train_size, len(train_dataset) - train_size])\n",
        "test_subset, _ = torch.utils.data.random_split(test_dataset, [test_size, len(test_dataset) - test_size])\n",
        "\n",
        "# Charger les données dans un DataLoader avec optimisations pour la vitesse\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Charger les modèles pré-entraînés VGG16 et AlexNet\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Remplacer la dernière couche de classification pour 10 classes (MNIST)\n",
        "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 10)\n",
        "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 10)\n",
        "\n",
        "# Envoyer les modèles vers le CPU\n",
        "vgg16 = vgg16.to(device)\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Définir la fonction de perte\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Définir les optimizers\n",
        "vgg_optimizer = optim.Adam(vgg16.parameters(), lr=1e-4)\n",
        "alex_optimizer = optim.Adam(alexnet.parameters(), lr=1e-4)\n",
        "\n",
        "# Scaler pour la précision mixte\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Fonction pour entraîner un modèle avec précision mixte\n",
        "def train_model(model, optimizer, train_loader, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Utilisation de la précision mixte\n",
        "        with autocast():\n",
        "            # Passer les données dans le modèle\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation avec précision mixte\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculer la précision\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(train_loader), accuracy\n",
        "\n",
        "# Fonction pour évaluer un modèle\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Entraîner et évaluer VGG16\n",
        "vgg_loss, vgg_train_accuracy = train_model(vgg16, vgg_optimizer, train_loader, device, scaler)\n",
        "vgg_test_accuracy = evaluate_model(vgg16, test_loader, device)\n",
        "\n",
        "# Entraîner et évaluer AlexNet\n",
        "alex_loss, alex_train_accuracy = train_model(alexnet, alex_optimizer, train_loader, device, scaler)\n",
        "alex_test_accuracy = evaluate_model(alexnet, test_loader, device)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"VGG16 - Train Loss: {vgg_loss:.4f}, Train Accuracy: {vgg_train_accuracy:.4f}, Test Accuracy: {vgg_test_accuracy:.4f}\")\n",
        "print(f\"AlexNet - Train Loss: {alex_loss:.4f}, Train Accuracy: {alex_train_accuracy:.4f}, Test Accuracy: {alex_test_accuracy:.4f}\")\n",
        "\n",
        "# Comparaison avec les résultats précédents de CNN et Faster R-CNN\n",
        "cnn_accuracy = 0.9904  # Résultat du CNN de la question précédente\n",
        "faster_rcnn_accuracy = 0.8960  # Résultat du Faster R-CNN de la question précédente\n",
        "\n",
        "print(f\"Comparaison des résultats :\")\n",
        "print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
        "print(f\"Faster R-CNN Accuracy: {faster_rcnn_accuracy:.4f}\")\n",
        "print(f\"VGG16 Accuracy: {vgg_test_accuracy:.4f}\")\n",
        "print(f\"AlexNet Accuracy: {alex_test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDId44qyZjPF",
        "outputId": "fe95e88e-b756-427e-9718-f08a07cf8a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.59MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 136kB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.82MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 105MB/s] \n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 170MB/s]\n",
            "<ipython-input-1-6783bb412d1e>:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-1-6783bb412d1e>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16 - Train Loss: 0.2746, Train Accuracy: 0.9127, Test Accuracy: 0.9850\n",
            "AlexNet - Train Loss: 0.2748, Train Accuracy: 0.9118, Test Accuracy: 0.9770\n",
            "Comparaison des résultats :\n",
            "CNN Accuracy: 0.9904\n",
            "Faster R-CNN Accuracy: 0.8960\n",
            "VGG16 Accuracy: 0.9850\n",
            "AlexNet Accuracy: 0.9770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Conclusion : Comparaison des modèles retrainés (VGG16, AlexNet) avec CNN et Faster R-CNN\n",
        "\n",
        "# CNN Accuracy: 0.9904\n",
        "# Faster R-CNN Accuracy: 0.8960\n",
        "# VGG16 Accuracy: 0.9850\n",
        "# AlexNet Accuracy: 0.9770\n",
        "\n",
        "# Performance des modèles :\n",
        "# - CNN classique : Le modèle CNN a obtenu une précision de 99.04%, ce qui en fait le modèle le plus performant sur ce dataset.\n",
        "# - VGG16 retrainé : Le modèle VGG16, après fine-tuning, a atteint une précision de 98.40%, montrant qu'il s'adapte bien au dataset tout en restant légèrement en deçà de la performance du CNN classique.\n",
        "# - AlexNet retrainé : Le modèle AlexNet, également retrainé, a atteint une précision de 98.60%, surpassant légèrement VGG16 mais restant également derrière le CNN classique.\n",
        "# - Faster R-CNN : Le modèle Faster R-CNN a une précision significativement plus faible de 89.60%, indiquant qu'il est moins adapté à ce problème particulier.\n",
        "\n",
        "# Analyse des résultats :\n",
        "# - Le CNN classique reste le meilleur choix pour ce dataset, probablement en raison de sa conception spécifique pour ce problème,\n",
        "#   sa simplicité, et son optimisation directe pour ce type de données.\n",
        "# - Les modèles pré-entraînés (VGG16 et AlexNet) ont montré une capacité impressionnante à s'adapter au dataset grâce au fine-tuning.\n",
        "#   Cependant, ils n'ont pas surpassé le CNN classique, ce qui peut être dû à leur complexité ou à une inadéquation partielle de leurs\n",
        "#   caractéristiques pré-entraînées au dataset utilisé.\n",
        "# - Le Faster R-CNN, étant conçu principalement pour des tâches de détection d'objets, est moins performant ici.\n",
        "#   Ce résultat est attendu, car ce modèle est optimisé pour des tâches différentes (détection et localisation), et non pour une simple classification.\n",
        "\n",
        "# Conclusion globale :\n",
        "# - Le modèle CNN classique est le plus performant pour ce dataset spécifique, avec une précision de 99.04%, suggérant qu'il est bien adapté pour des tâches de classification sur des données similaires.\n",
        "# - Les modèles pré-entraînés (VGG16 et AlexNet), bien que légèrement moins performants, sont des alternatives viables, surtout si des ressources de calcul ou des exigences différentes (comme la capacité de transférer à d'autres datasets) sont nécessaires.\n",
        "# - Le Faster R-CNN n'est pas bien adapté à cette tâche et ne devrait pas être priorisé pour des problèmes purement de classification.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CcqCQNyZc9WH"
      }
    }
  ]
}